#another option is to loqd the data in Unit1_piechart.csv
#two options:wirte
data <- read.csv2("data_csv_files/Unit1_Piechart.csv", header = T)
sport_vector
#Declare vectors: values of the variable and frequencies
sport_vector <- data$Favourite_Sport
sport_vector
frequency_vector <- data$Frequency
frequency_vector
#now depict a piechart
pie(frequency_vector, labels = sport_vector, main = "Pie charts of favorite sport")
data_height <- read.csv("data_csv_files/Unit1_Heights.csv", header = T)
View(data_height)
data
data_height
#another way of creating varaibles is bu indicating the column:
Heights <- data[,1]
Heights
#another way of creating varaibles is bu indicating the column:
Heights <- data_height[,1]
Heights
#now compute the mean
mean(Heights)
#and the median
median(Heights)
quantile(Heights)
data_height$Heights
#or a specific percentile
quantile(Heights,0.25)
#and a histogram (with some of the many options available)
hist(Heights, breaks = (1.6, 1.7, 1.8, 1.9), col="blue", main="Histogram",
xlab="Height", ylab="Frequency")
#and a histogram (with some of the many options available)
hist(Heights, breaks = (1.6, 1.7, 1.8, 1.9), col="blue", main="Histogram",
xlab="Height", ylab="Frequency")
#and a histogram (with some of the many options available)
hist(Heights, breaks = c(1.6, 1.7, 1.8, 1.9), col="blue", main="Histogram",
xlab="Height", ylab="Frequency")
hist(Heights,col="blue",main="histogram",xlab="Height", ylab = "Frequency")
range_Heights <- max(Heights)
range_Heights
range_Heights <- max(Heights) - min(Heights)
range_Heights
IQR(Heights)
quantile(Heights)
#standard deviation
sd(Heights)
#variance
var(Heights)
#coefficient of variation.
CV <- function(mead, sd){
(sd/mean)*100
}
CV(mean=mean(Heights), sd=sd(Heights))
CV(3,2)
CV(mean= mean(Heights), sd=sd(Heights))
#coefficient of variation.
CV <- function(mean, sd){
(sd/mean)*100
}
CV(mean= mean(Heights), sd=sd(Heights))
CV(3,2)
#Skewness(asymetry) coefficient and Kurtosis
library(moments)
#Skewness(asymetry) coefficient and Kurtosis
install.packages(moments)
library(moments)
#Skewness(asymetry) coefficient and Kurtosis
install.packages(moments)
install.packages(moments)
#Skewness(asymetry) coefficient and Kurtosis
install.packages("moments")
library(moments)
skewness(Heights)
kurtosis(Heights)
boxplot(Heights, horizontal = TRUE, main="Box-Whisker plot",
xlab="Heights", ylab="values")
abline(v=mean(Heights), col="red")
read.csv("data_csv_files/Data_Expenses.csv",header = T)
data_expenses <- read.csv("data_csv_files/Data_Expenses.csv",header = T)
data_expenses <- read.csv("data_csv_files/Data_Expenses.csv",header = T, sep=";")
View(data_expenses)
length(data)
length(data_expenses)
length(data_expenses$ExpensesMay)
mean(data_expenses$ExpensesMay)
data_expenses <- read.csv("data_csv_files/Data_Expenses.csv",header = T, sep=";", quote = ",")
data_expenses <- read.csv("data_csv_files/Data_Expenses.csv",header = T, sep=";", quote = ",")
data_expenses <- read.csv("data_csv_files/Data_Expenses.csv",header = T, sep=";", dec = ",")
length(data_expenses$ExpensesMay)
mean(data_expenses$ExpensesMay)
median(data_expenses$ExpensesMay)
quantile(data_expenses$ExpensesMay, 0.25)
quantitle(data_expenses$ExpensesMay, 0.75)
quantile(data_expenses$ExpensesMay, 0.75)
may <- data_expenses$ExpensesMay
var(may)
sd(may)
max(may)-min(may)
q1<-quantile(data_expenses$ExpensesMay, 0.25)
q3<-quantile(data_expenses$ExpensesMay, 0.75)
q3-q1
IQR(may)
quantitle(may,0.85)
quantile(may,0.85)
1-pbinom(2, size = 10, prob = 0.02)
1-pbinom(0, size = 5, prob = 0.1)
#exe find n2
pbinom(1, size = 10, prob=0.05)
#exe find n2
pbinom(1, size = 50, prob=0.05)
#exe find n2
pbinom(1, size = 100, prob=0.05)
#exe find n2
pbinom(1, size = 75, prob=0.05)
#exe find n2
pbinom(1, size = 85, prob=0.05)
#exe find n2
pbinom(1, size = 80, prob=0.05)
#exe find n2
pbinom(1, size = 77, prob=0.05)
#exe find n2
pbinom(1, size = 76, prob=0.05)
#normal distribution
#in this file the decimal separator is '.' so we use read.csv
data_height -> read.csv("data_csv_files/Unit1_Heights.csv", header = T)
#normal distribution
#in this file the decimal separator is '.' so we use read.csv
data_height <- read.csv("data_csv_files/Unit1_Heights.csv", header = T)
height <- data_height$Heights
#find the normal probability plot
qqnorm(height)
#do the shapiro-wilk test
shapiro.test(height)
pnrom(4.9, mean=5, sd=0.1, lowe.tail=true)
pnorm(4.9, mean=5, sd=0.1, lowe.tail=true)
pnorm(4.9, mean=5, sd=0.1, lower.tail=true)
pnorm(4.9, mean=5, sd=0.1, lower.tail=ture)
pnorm(4.9, mean=5, sd=0.1, lower.tail=TRUE)
pnorm(4.8, mean=5, sd=0.1, lower.tail=FALSE)
pnorm(45, mean=25, sd=7, lower.tail=TRUE)
pnorm(45, mean=30, sd=3, lower.tail=TRUE)
pnorm(60, mean=30, sd=3, lower.tail=TRUE)
pnorm(50, mean=30, sd=3, lower.tail=TRUE)
pnorm(30, mean=30, sd=3, lower.tail=TRUE)
pnorm(40, mean=30, sd=3, lower.tail=TRUE)
pnorm(35, mean=30, sd=3, lower.tail=TRUE)
pnorm(32, mean=30, sd=3, lower.tail=TRUE)
pnorm(33, mean=30, sd=3, lower.tail=TRUE)
pnorm(34, mean=30, sd=3, lower.tail=TRUE)
pbinom(5, size = 100, prob = 0.08)
pbinom(5, size = 500, prob = 0.08)
pbinom(5, size = 250, prob = 0.08)
pbinom(5, size = 200, prob = 0.08)
pbinom(5, size = 150, prob = 0.08)
pbinom(5, size = 125, prob = 0.08)
pbinom(5, size = 110, prob = 0.08)
pbinom(5, size = 115, prob = 0.08)
pbinom(5, size = 112, prob = 0.08)
pbinom(5, size = 111, prob = 0.08)
pbinom(5, size = 110, prob = 0.08)
pbinom(5, size = 113, prob = 0.08)
pbinom(5, size = 114, prob = 0.08)
#first exercise about confidence intervals for the mean
income <- c(125,135,145,130,120,145,125,130,150,145)
mean(income)
sd(income)
#this conmmand directly computes the confidence interval
t.test(income, conf.level = 0.95)
#when sigma is known
install.packages('TeachingDemos')
#when sigma is known
install.packages('TeachingDemos')
prop.test(120,200, conf.level = 0.95)
#example
before <- c(26.2,20.3,25.4,19.6,21.5,28.3,23.7,24)
after <-c(24.1,21.3,23.7,18,20.1,25.8,22.4,21.4)
difference <- before-after
t.test(difference)
t.test(difference, conf.level = 1)
t.test(difference, conf.level = 0)
#exemple
speed <- c(69,60,80,85,68,74,60,86,92)
t.test(speed, mu=70)
t.test(speed, mu=70, alternative = "less")
t.test(speed, mu=70, alternative = "more")
t.test(speed, mu=70, alternative = "greater")
#unites produces x:defective
prop.test(2,20,p=0.05,alternative = "two.sided")
prop.test(20,200,p=0.05,alternative = "two.sided")
#technological companies
prop.test(6,200,p=0.02, alternative = "greater")
#technological companies
prop.test(60,2000,p=0.02, alternative = "greater")
country_A <- c(200,230,205,185,190,300,250,245,208)
country_B <- c(190,220,200,180,190,260,260,240,241,200)
var.test(country_A,country_B)
country_B <- c(190,220,200,180,190,260,240,241,200)
var.test(country_A,country_B)
t.test(country_A, country_B, var.equal = TRUE)
#paired sample
t.test(before, after, paired = TRUE)
#paired sample
t.test(country_A, country_B, paired = TRUE)
#lab
data_expenses <- read.csv("data_csv_files/Data_Expenses.csv",header = T, sep=";", dec = ",")
View(data_expenses)
May <- data_expenses$ExpensesMay
May
View(data_expenses)
t.test(May,conf.level = 0.95)
t.test(May,mu=100,alternative = "greater")
June <- data_expenses$ExpensesJune
t.test(May,June, paired = TRUE, alternative = "two.sided")
#P7 exemple
data_resources <- read.csv2("./data_csv_files/Unit5_Resource.csv",header = T)
#P7 exemple
data_resources <- read.csv2("./data_csv_files/Unit5_Resources.csv",header = T)
View(data_resources)
Resource <- data_resources$Resource
Price <- data_resources$Price
#Treat resource as a factor
Resource_Factor <- as.factor(Resource)
#get the ANOVA table
anova_resources <- aov(lm(Price~Resource_Factor))
summary(anova_resources)
#
require(graphics)
plot(TukeyHSD(anova_resources),"Resource_Factor",ordered=T)
plot(TukeyHSD(anova_resources),"Resource_Factor",ordered=TRUE)
plot(TukeyHSD(anova_resources),"Resource_Factor ",ordered=TRUE)
plot(TukeyHSD(anova_resources),"Resource_Factor",ordered=TRUE)
#Treat resource as a factor
Resource_Factor<- as.factor(Resource)
plot(TukeyHSD(anova_resources),"Resource_Factor",ordered=TRUE)
plot(TukeyHSD(anova_resources), "Resource_Factor",ordered=TRUE)
plot(TukeyHSD(anova_resources,"Resource_Factor",ordered=TRUE))
#P12 example
data_co2 <- read.csv2("./data_csv_files/Unit5_CO2.csv",header = T)
View(data_co2)
CO2<-data_co2$CO2
Machine<-data_co2$Machine
Machine_Factor <- as.factor(Machine)
anova_machine <- aov(lm(CO2~Machine_Factor))
summary(anova_machine)
require(grqphics)
require(graphics)
plot(TukeyHSD(anova_machine,"Machine_Factor",ordered = T))
#P12 example
data_co2 <- read.csv2("./data_csv_files/Unit5_CO2.csv",header = T)
CO2<-data_co2$CO2
Machine<-data_co2$Machine
Machine_Factor <- as.factor(Machine)
anova_machine <- aov(lm(CO2~Machine_Factor))
summary(anova_machine)
#two factors
data_computer<-read.csv2("./data_csv_files/Unit5_Computers.csv",header = T)
View(data_computer)
Card <- data_computer$CARD
Speed <- data_computer$SPEED
Performance <- data_computer$Performance
Card_Factor <- as.factor(Card)
Speed_Factor <- as.factor(Speed)
anova_two_factors <- aov(lm(Performance~Card_Factor*Speed_Factor))
summary(anova_two_factors)
summary(anova_two_factors)
#the two factors and the interaction are significant
#now try to show the means plot
#table of means
aggregate(Performance~Card_Factor+Speed_Factor, FUN = "mean")
#interaction plot: performance as a function of speed, for each value
interaction.plot(Speed_Factor,Card_Factor,Performance,
xlab = "Speed_Factor",
ylab = "Performance",
trace.label = "Card_Factor",
main="Interaction Plot",
col = c("blue","red","green"),
bg=c("bleu","red","green"),
pch = c(18,24,22),
type = "b")
#interaction plot: performance as a function of speed, for each value
interaction.plot(Speed_Factor,Card_Factor,Performance,
xlab = "Speed_Factor",
ylab = "Performance",
trace.label = "Card_Factor",
main="Interaction Plot",
col = c("blue","red","green"),
bg=c("blue","red","green"),
pch = c(18,24,22),
type = "b")
#two factors
data_computer<-read.csv2("./data_csv_files/Unit5_Computers.csv",header = T)
Card <- data_computer$CARD
Speed <- data_computer$SPEED
Performance <- data_computer$Performance
Card_Factor <- as.factor(Card)
Speed_Factor <- as.factor(Speed)
anova_two_factors <- aov(lm(Performance~Card_Factor*Speed_Factor))
summary(anova_two_factors)
#the two factors and the interaction are significant
#now try to show the means plot
#table of means
aggregate(Performance~Card_Factor+Speed_Factor, FUN = "mean")
#interaction plot: performance as a function of speed, for each value
interaction.plot(Speed_Factor,Card_Factor,Performance,
xlab = "Speed_Factor",
ylab = "Performance",
trace.label = "Card_Factor",
main="Interaction Plot",
col = c("blue","red","green"),
bg=c("blue","red","green"),
pch = c(18,24,22),
type = "b")
help("interaction.plot")
#exercise
data_resistance <- read.csv2("./data_csv_files/Unit5_Resistance.csv", header = T)
View(data_resistance)
A <- data_resistance$A
B <- data_resistance$B
Resistance <- data_resistance$Resistance
A_Factor <- as.factor(A)
B_Factor <- as.factor(B)
anova_resistance <- aov(lm(Resistance~A_Factor*B_Factor))
summary(anova_resistance)
#Pa < alpha => A is significant
#Pb < alpha => B is significant
#Pab > alpha => interaction isn't significant
aggregate(Resistance~A_Factor*B_Factor, FUN = "mean")
interaction.plot(A_Factor, B_Factor, Resistance,
xlab = "A",
ylab = "Resistance",
trace.label = "B",
main="Interaction Plot",
col = c("blue","red","green"),
bg=c("blue","red","green"),
pch = c(18,24,22),
type = "b")
#lab session
data_shop <- read.csv2("./data_csv_files/Data_Shops.csv", header = T)
View(data_shop)
Profits <- data_shop$Profits
Size <- data_shop$Size
Location <- data_shop$Location
Size_Factor <- as.factor(Size)
Location_Factor <- as.factor(Location)
anova_profits <- aov(lm(Profits~Size_Factor*Location_Factor))
summary(anova_profits)
interaction.plot(Location_Factor, Size_Factor, Profits,
xlab = "Location",
ylab = "Profits",
trace.label = "Size",
main="Interaction Plot",
col = c("blue","red","green"),
bg=c("blue","red","green"),
pch = c(18,24,22),
type = "b")
data_lifespan <- read.csv2("./data_csv_files/Unit6_Lifespan.csv", header = T)
View(data_lifespan)
View(data_lifespan)
#define variable
Lifespan <- data_lifespan$Lifespan
Price <- data_lifespan$Price
Use <- data_lifespan$Use
model_lifespan <- lm(Lifespan~Price+Use)
View(model_lifespan)
help(lm)
#help(lm)
summary(model_lifespan)
#Prediction
predict.lm(model_lifespan, data.frame(Price=10, Use=0.5))
predict.lm(model_lifespan, data.frame(Price=10, Use=0.5), interval = "confidence")
#confidence interval for the one value
predict.lm(model_lifespan, data.frame(Price=10, Use=0.5), interval = "prediction")
#help(lm)
summary(model_lifespan)
random(305)
runif(1, 1, 305)
runif(1, 1, 305)
runif(1, 1, 305)
model2 <- lm(Lifespan~Price)
summary(model2)
#Voltage
data_voltage <- read.csv2("./data_csv_files/Unit6_Voltage.csv", header = T)
View(data_voltage)
#define variables
Voltage <- data_voltage$Voltage
Current <- data_voltage$Current
#first draw a scatter plot to try and guess the relationship between the variables
plot(Voltage~Current, main="Dispersion graph", xlab="Voltage", ylab = "Current", col="blue")
#it looks like a logrithmic model couls apply
model_log <- lm(Voltage~log(Current + 0.01))
summary(model_log)
#other posibilities
summary(lm(Voltage~Current))
#double-log model
summary(lm(log(Voltage+0.01)~log(Current+0.01)))
#lab session
data_gdp <- read.csv2("./data_csv_files/Data_GDP.csv", header = T)
#lab session
data_gdp <- read.csv2("./data_csv_files/Data_GDP.csv", header = T)
View(data_gdp)
Year <- data_gdp$Year
UR <- data_gdp$UR
GDP <- data_gdp$GDP
plot(GDP~UR, main="Dispersion graph", xlab="gdp", ylab = "ur", col="blue")
model_ln <- lm(GDP~UR)
summary(model_ln)
mean(GDP)
mean(UR)
summary(lm(UR~GDP))
mean(UR)/mean(GDP)
model_ln <- lm(UR~GDP)
summary(model_ln)
plot(UR~GDP, main="Dispersion graph", xlab="gdp", ylab = "ur", col="blue")
predict.lm(model_ln,data.frame(GDP=180),interval = "confidence")
help("predict.lm")
install.packages("arima")
data_passengers <- read.csv2("./data_csv_files/Unit7_Passengers.csv",header = T)
View(data_passengers)
data_passengers.head()
users <- data_passengers$USERS
Month <- data_passengers$MONTH
Users <- data_passengers$USERS
#Unit 7
#Train Passengers
data_passengers <- read.csv2("./data_csv_files/Unit7_Passengers.csv",header = T)
Users <- data_passengers$USERS
Month <- data_passengers$MONTH
ts_Users <- ts(Users, frequency = 12, start = c(1989.1), end = c(2015.12))
ts_Users <- ts(Users, frequency = 12, start = c(1989,1), end = c(2015,12))
#plot the time series
plot(ts_Users,main="Time Series Plot", type="l",lwd=2,col="blue",xlab="Month-Year", ylab="Users")
help(plot)
plot(stl(ts_Users,s.window = "periodic"))
help(s.window)
??s.window
help(stl)
#arima
install.packages("forecast")
library(forecast)
model_best <- auto.arima(ts_Users)
model_best
#find the forecasts for the next 9 months
predictions <- predict(model_best,n.ahead = 9)
predictions
View(predictions)
plot(predictions$pred, type="l",col="blue")
#find the forecasts for the next 9 months
predictions <- predict(model_best,n.ahead = 12)
plot(predictions$pred, type="l",col="blue")
predictions
plot(ts_Users+predictions$pred, type="l",col="blue")
plot(predictions$pred, type="l",col="blue")
union <- ts.union(ts_Users, predictions$pred)
plot(union, type="l",col="blue")
View(union)
View(union)
ts_Users.append(predictions$pred)
#find the forecasts for the next 9 months
predictions <- predict(model_best,n.ahead = 9)
predictions
plot(predictions$pred, type="l",col="blue")
ts.plot(ts_Users, predictions$pred,lty=c(1,3),col=c(5,2))
#find the forecasts for the next 9 months
predictions <- predict(model_best,n.ahead = 12)
ts.plot(ts_Users, predictions$pred,lty=c(1,3),col=c(5,2))
help("ts.plot")
ts.plot(ts_Users, predictions$pred,lty=c(1:3),col=c(5,2))
#plot the series plus the prediction
ts.plot(ts_Users, predictions$pred,lty=c(1,3),col=c("black",2))
#plot the series plus the prediction
ts.plot(ts_Users, predictions$pred,lty=c(1,3),col=c(3,2))
#1.96 is the critical point of a Normal(0,1) needed for the confidencde interval
upper <- predictions$pred + 1.96*predictions$se
lower <- predictions$pred - 1.96*predictions$se
plot(predictions$pred, col="blue")
lines(upper, col="red", lty=3)
lints(lower, col="red", lty=3)
lines(lower, col="red", lty=3)
actuel <- c(6441.1, 6446.6, 7466.3, 7652.31, 7835.89, 8154.57, 8712.82, 8706.77,.8762.54)
actuel <- c(6441.1, 6446.6, 7466.3, 7652.31, 7835.89, 8154.57, 8712.82, 8706.77,8762.54)
ts_actual <- ts(actual, frequency = 12, start = c(2016,1), end = c(2016,9))
actual <- c(6441.1, 6446.6, 7466.3, 7652.31, 7835.89, 8154.57, 8712.82, 8706.77,8762.54)
ts_actual <- ts(actual, frequency = 12, start = c(2016,1), end = c(2016,9))
plot(ts_actual, col="red", type="p", ylim=c(6000,9000))
lines(upper,col="blue", lty=3)
lines(lower,col="blue",lty=3)
points(predictions$pred,col="blue")
predictions_forecast <- forecast(model_best, h=9, level = 0.95)
plot(predictions_forecast)
predictions_forecast
#predictions using function forecast
predictions_forecast <- forecast(model_best, h=24, level = 0.95)
plot(predictions_forecast)
#predictions using function forecast
predictions_forecast <- forecast(model_best, h=9, level = 0.95)
plot(predictions_forecast)
predicted_values <- predictions_forecast$mean
mean
predicted_values
predictied_lower <- predictions_forecast$lower
predictied_lower
predictied_upper <- predictions_forecast$upper
predictied_upper
#assess the accuaracy of the model
aacuracy(model_best)
#assess the accuaracy of the model
accuracy(model_best)
accuracy(predictions$pred, ts_actual)
model_best
data_passengers <- read.csv2("./data_csv_files/Data_Consumption.csv",header = T)
View(data_passengers)
